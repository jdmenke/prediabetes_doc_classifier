{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9141638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 213kB [00:00, 19.8MB/s]\n",
      "2023-05-01 15:27:09 INFO: Downloading these customized packages for language: en (English)...\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | genia   |\n",
      "| pos       | genia   |\n",
      "| lemma     | genia   |\n",
      "| depparse  | genia   |\n",
      "| pretrain  | genia   |\n",
      "=======================\n",
      "\n",
      "2023-05-01 15:27:09 INFO: File exists: /Users/joemenke/stanza_resources/en/tokenize/genia.pt\n",
      "2023-05-01 15:27:09 INFO: File exists: /Users/joemenke/stanza_resources/en/pos/genia.pt\n",
      "2023-05-01 15:27:09 INFO: File exists: /Users/joemenke/stanza_resources/en/lemma/genia.pt\n",
      "2023-05-01 15:27:09 INFO: File exists: /Users/joemenke/stanza_resources/en/depparse/genia.pt\n",
      "2023-05-01 15:27:10 INFO: File exists: /Users/joemenke/stanza_resources/en/pretrain/genia.pt\n",
      "2023-05-01 15:27:10 INFO: Finished downloading models and saved to /Users/joemenke/stanza_resources.\n",
      "2023-05-01 15:27:10 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 213kB [00:00, 17.4MB/s]\n",
      "2023-05-01 15:27:10 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | genia   |\n",
      "| pos       | genia   |\n",
      "| lemma     | genia   |\n",
      "| depparse  | genia   |\n",
      "=======================\n",
      "\n",
      "2023-05-01 15:27:10 INFO: Using device: cpu\n",
      "2023-05-01 15:27:10 INFO: Loading: tokenize\n",
      "2023-05-01 15:27:10 INFO: Loading: pos\n",
      "2023-05-01 15:27:11 INFO: Loading: lemma\n",
      "2023-05-01 15:27:11 INFO: Loading: depparse\n",
      "2023-05-01 15:27:11 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer,AutoTokenizer,AutoModelForSequenceClassification, set_seed\n",
    "from datasets import Dataset,load_dataset, load_from_disk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from run_normalize_sections import normalize_section\n",
    "import stanza\n",
    "\n",
    "stanza.download('en', package='genia')\n",
    "nlp = stanza.Pipeline('en', package='genia')\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eff44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c68ff4",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ef110f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paper(inputs):\n",
    "    with open(inputs) as fp:\n",
    "        contents = fp.read()\n",
    "        paper_dict = json.loads(contents)\n",
    "        normalized_dict = normalize_section(paper_dict)\n",
    "    return normalized_dict\n",
    "\n",
    "def directory_looper(directory):\n",
    "    input_list=[]\n",
    "    filenames = []\n",
    "    for name in os.listdir(directory):\n",
    "        filename = os.fsdecode(name)\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_list.append(os.path.join(directory, filename))\n",
    "            filenames.append(filename)\n",
    "    return input_list, filenames\n",
    "\n",
    "def process_directory(input_directory):\n",
    "    inputs, filenames = directory_looper(input_directory)\n",
    "    data = []\n",
    "    for i in range(len(inputs)):\n",
    "        norm_data = process_paper(inputs[i])\n",
    "        norm_data[\"filename\"] = filenames[i]\n",
    "        data.append(norm_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fb86e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list2str(row):\n",
    "    try:\n",
    "        row = ast.literal_eval(row)\n",
    "    except:\n",
    "        row = row.strip('][').replace(\"'\", \"\").split(', ')\n",
    "    return \" \".join(row)\n",
    "\n",
    "def split_sentences(row):\n",
    "    doc = nlp(row) # nlp is the biomedical stanza model initialized in the import cell\n",
    "    return [sentence.text for sentence in doc.sentences]\n",
    "\n",
    "def process_text(df, col_name = 'methods'):\n",
    "    df_ = df[['filename', col_name]]\n",
    "    df_ = df_.dropna()\n",
    "    df_[col_name] = df_[col_name].apply(convert_list2str)\n",
    "    df_[col_name] = df_[col_name].progress_apply(split_sentences)\n",
    "    df_ = df_.explode(col_name)\n",
    "    df_.rename(columns = {col_name:'text'}, inplace = True)\n",
    "    return df_\n",
    "\n",
    "def process_sections(infile_loc, sections2check):\n",
    "    infile = process_directory(infile_loc)\n",
    "    df = pd.DataFrame.from_records(infile)\n",
    "    sections = []\n",
    "    for section in sections2check:\n",
    "        sections.append(process_text(df, section))\n",
    "    sections = pd.concat(sections)\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94729ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = ['abstract', 'intro', 'methods', 'results', 'discussion', 'conclusions']\n",
    "\n",
    "cai_text = process_sections('/data/cai/txt', sections) # Cai Prediabetes Meta-Analysis\n",
    "guj_text = process_sections('/data/gujral/txt', sections) # Gujral Prediabetes Meta-Analysis\n",
    "\n",
    "frames = [cai_text, guj_text]\n",
    "text = pd.concat(frames)\n",
    "\n",
    "unique_text = text.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2d3b68",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8f27d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"biolink_model\"\n",
    "tokenizer_name = \"michiyasunaga/BioLinkBERT-base\"\n",
    "max_length = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "def tokenize_function(data):\n",
    "    return tokenizer(data[\"text\"], padding='max_length', truncation=True, max_length = max_length)\n",
    "\n",
    "def preprocessing(dataset):\n",
    "    dataset = Dataset.from_pandas(dataset[['text']], preserve_index=False)\n",
    "    dataset_token = dataset.map(tokenize_function)\n",
    "    return dataset_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2800ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joemenke/si630_pro/lib/python3.7/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_dataset = preprocessing(unique_text)\n",
    "\n",
    "biolink_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 2).to(device)\n",
    "trainer = Trainer(model = biolink_model)\n",
    "\n",
    "text_pred = trainer.predict(text_dataset)\n",
    "\n",
    "predictions = list(np.argmax(text_pred.predictions.squeeze(), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "84814362",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_dataset['text']\n",
    "filenames = unique_text['filename']\n",
    "\n",
    "text_dict = {'text': text, 'name': filenames, 'prediction': predictions} \n",
    "    \n",
    "text_df = pd.DataFrame(text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5a2e1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv('/data/doc_summary_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253eb83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si630_pro",
   "language": "python",
   "name": "si630_pro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
